\documentclass[12pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{array}
\usepackage{tcolorbox}
\usepackage{booktabs}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{algorithmic}
\geometry{a4paper, margin=1in}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{remark}[theorem]{Remark}
\lstset{
    language=Python,
    frame=single,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue}\bfseries,
    commentstyle=\color{green!50!black},
    stringstyle=\color{orange},
    numbers=left,
    numberstyle=\tiny,
    numbersep=5pt,
    showstringspaces=false,
    breaklines=true,
    captionpos=b,
    escapeinside={(*@}{@*)}
}
\newtcbox{\inlinecode}{on line,
  colback=gray!10, colframe=gray!50,
  boxrule=0.5mm, sharp corners=all, boxsep=0mm, left=2pt, right=2pt, top=2pt, bottom=2pt}

\begin{document}


\begin{titlepage}
    \begin{center}
        {\Huge \textbf{A Comparative Study of Some iterative Methods for Solving Linear Systems}} \\[1cm]
        {\Large A. OUARDI} \\[1cm]
        {\Large \today} \\[2cm]

        {\Large \textbf{Abstract}} \\[0.5cm]
        This document provides a detailed discussion on several algorithms, including their implementation, analysis to solve linear algebraic equations. The iterative methods considered are the Steepest-descent Gradient (SDG), the Conjugate Gradient (CG), and the Krylov subspace methods. \\[2cm]

        %{\Large \textbf{Table of Contents}} \\[0.5cm]
        %\addtocontents{toc}{\protect\setcounter{tocdepth}{2}}
        \tableofcontents
    \end{center}
\end{titlepage}

\newpage


\section{Introduction}
Solving linear systems is a fundamental problem in scientific computing and engineering applications. We consider the nonsingular system of $n$ linear algebraic equations \begin{equation}\label{sys1}
    A x=b.
\end{equation} Direct methods such as Gaussian elimination and LU decomposition are robust but computationally expensive for large systems because their complexity is $O(N^3)$, while iterative methods, such as Steepest Descent (SD), Conjugate Gradient (CG),  Full Orthogonalisation (FOM) algorithm and Generalized Minimal Residual method (GMRES) are attractive and offer efficient alternatives for sparse and large-scale systems. This report explores these algorithms, providing their theoretical background, Python implementations, and numerical results. We also discuss their advantages, limitations, and possible extensions.
\section{Description of algorithms}
\subsection{Steepest Descent}
Let's consider the linear system of equations \ref{sys1}, we assume that $A$ is an $n \times n$ nonsingular, real symmetric and positive definite matrix. We define the function $\Phi : \mathbb{R^n} \mapsto \mathbb{R}$, as
$$
\Phi(x):=\frac{1}{2}\langle A x, x\rangle-\langle b, x\rangle
$$

where $\langle.,$.$\rangle$ is the usual euclidian scalar product.
\begin{theorem}
    Assuming that $A$ is positive definite, we have

$$
\Phi\left(x^*\right)=\min _{x \in \mathbb{R}^n} \Phi(x) \Longleftrightarrow A x^*=b
$$

where $x^*$ is the exact solution of the problem \ref{sys1}.
\end{theorem}
Based on the classical well known steepest descent methods, we will present the derivation of this methods for solving numerically the problem \ref{sys1}. Starting from a given initial guess $x_0$, and assuming that at step $k$, we have an approximation $x_k$. Then the next approximation $x_{k+1}$ is constructed as follows: choose a direction vector $p_k$ (known as descent direction) and look for the new iteration $x_{k+1}$ as

\begin{equation}\label{grad}
x_{k+1}=x_k+\alpha_k p_k
\end{equation}

where $\alpha_k$ is the parameter to be computes such that $x_{k+1}-x^*$ is minimized. Which gives the following algorithm 
\begin{algorithm}[h]
\caption{Steepest Descent}
\label{alg:steepest_gradient}
\begin{algorithmic}[1]
\REQUIRE Matrix $A$, vector $b$, max iterations $\text{iter\_max}$, tolerance $\text{tol}$, initial guess $x = 0$
\STATE $r = b-Ax$, $\text{err} = \|r\|$
\STATE Initialize residual list $L = [\text{err}]$, $\text{iter} = 0$
\WHILE{$\text{iter} < \text{iter\_max}$ \AND $\text{err} > \text{tol}$}
    \STATE $\text{iter} = \text{iter} + 1$
    \STATE $A r = A \cdot r$
    \STATE $\text{err2} = r^\top r$
    \STATE $\alpha = \frac{\text{err2}}{(A r)^\top r}$
    \STATE $x = x + \alpha \cdot r$
    \STATE $r = r - \alpha \cdot (A r)$
    \STATE $\text{err} = \sqrt{\text{err2}}$
    \STATE Append $\text{err}$ to $L$
\ENDWHILE
\RETURN $L, \text{iter}$
\end{algorithmic}
\end{algorithm}
\subsection{Conjugate Gradient}
The iterate of the conjugate gradient method is still expressed as \ref{grad}. Here, the direction $p_k$ will be chosen in the plan generated by $r_k$ and $p_{k-1}$ as

$$
p_k=r_k+\beta p_{k-1}
$$

where the parameter $\beta$ will be selected such that the reduction in the A-error norm will be as large as possible as compared to $x_k-x^*$.

The conjugate gradient algorithm is given as follows
\begin{algorithm}[h]
\caption{Conjugate Gradient}
\label{alg:conjugate_gradient}
\begin{algorithmic}[1]
\REQUIRE Matrix $A$, vector $b$, max iterations $\text{iter\_max}$, tolerance $\text{tol}$, initial guess $x = 0$
\STATE $r = b-Ax$, $p = b$, $\text{err} = \|r\|$
\STATE Initialize residual list $L = [\text{err}]$, $\text{iter} = 0$
\WHILE{$\text{iter} < \text{iter\_max}$ \AND $\text{err} > \text{tol}$}
    \STATE $\text{iter} = \text{iter} + 1$
    \STATE $\text{rk2} = r^\top r$
    \STATE $\alpha = \frac{\text{rk2}}{(A p)^\top p}$
    \STATE $x = x + \alpha \cdot p$
    \STATE $r = r - \alpha \cdot (A p)$
    \STATE $\beta = \frac{r^\top r}{\text{rk2}}$
    \STATE $p = r + \beta \cdot p$
    \STATE Append $\|r\|$ to $L$
\ENDWHILE
\RETURN $L, \text{iter}, x$
\end{algorithmic}
\end{algorithm}
\subsection{Krylov subspace methods}
In this section, we will consider some iterative methods using Krylov subspace methods that are very effective projection methods onto low dimensional Krylov subspaces spanned by vectors of the form $p(A) v$ where $p$ is a polynomial. The idea comes from the fact that the exact solution of a linear system of equation can be expressed as a polynomial times the right hand side vector of the linear system. We will consider Krylov subspace projection methods based on the Arnoldi process, which is the Gram-Schmidt orthogonalisation process applied to Krylov bases of the form $\left\{v, A v, \ldots, A^m v\right\} = K_m(A,v).$
\subsubsection{FOM}.
The Full Orthogonalisation Method (FOM) seeks for an approximation $x_m$ such that
$$
x_m-x_0 \in K_m\left(A, r_0\right),
$$
with the orthogonality Galerkin condi
$$
r_m \perp K_m\left(A, r_0\right)
$$

\begin{algorithm}[h]
\caption{Arnoldi Method (FOM)}
\label{alg:arnoldi}
\begin{algorithmic}[1]
\REQUIRE Matrix $A$, vector $b$, subspace size $m$, tolerance $\text{tol}$, initial guess $x = 0$
\STATE Initialize $r = b$, $\|r\| = \text{norm}(r)$, $e_1 = [\|r\|, 0, \dots, 0]^\top$
\STATE Initialize $Q$ as $n \times (m+1)$ matrix with $Q[:, 0] = r / \|r\|$
\STATE Initialize $H$ as $(m+1) \times m$ zero matrix
\STATE Initialize residuals list
\FOR{$j = 0, 1, \dots, m-1$}
    \STATE $w = A \cdot Q[:, j]$
    \FOR{$i = 0, 1, \dots, j$}
        \STATE $H[i, j] = Q[:, i]^\top w$
        \STATE $w = w - H[i, j] \cdot Q[:, i]$
    \ENDFOR
    \STATE $H[j+1, j] = \|w\|$
    \IF{$H[j+1, j] == 0$}
        \STATE \textbf{break}
    \ENDIF
    \STATE $Q[:, j+1] = w / H[j+1, j]$
    \STATE Solve $H[:j+1, :j+1] y = e_1[:j+1]$ for $y$
    \STATE $x_m = Q[:, :j+1] y$
    \STATE $\text{residual} = \|b - A x_m\|$
    \IF{$\text{residual} < \text{tol}$}
        \RETURN $x_m, \text{residuals}$
    \ENDIF
    \STATE Append $\text{residual}$ to residuals list
\ENDFOR
\RETURN $x_m, \text{residuals}$
\end{algorithmic}
\end{algorithm}
\subsubsection{GMRES}
Now, we consider an orthogonal projection method named The Generalized Minimal Residual (GMRES) method, it is one of the well known and robust used Krylov solver for large problems. The approximate solution is obtained as follows
$$
x_m-x_0 \in K_m\left(A, r_0\right) \Longleftrightarrow x_m=x_0+V_m y
$$
with the orthogonality relation

$$
r_m \perp A K_m\left(A, r_0\right)
$$

\begin{algorithm}[h!]
\caption{GMRES Algorithm}
\label{alg:gmres}
\begin{algorithmic}[1]
\REQUIRE Matrix $A \in \mathbb{R}^{n \times n}$, vector $b \in \mathbb{R}^n$, tolerance $\text{tol}$, maximum iterations $\text{max\_iter}$, initial guess $x_0$
\STATE $r_0 = b - A x_0$, $\beta = \|r_0\|$
\IF{$\beta < \text{tol}$}
    \RETURN $x_0$
\ENDIF
\STATE Initialize $Q \in \mathbb{R}^{n \times (\text{max\_iter}+1)}$, $H \in \mathbb{R}^{(\text{max\_iter}+1) \times \text{max\_iter}}$
\STATE $Q[:, 0] = r_0 / \beta$
\FOR{$k = 0, 1, \dots, \text{max\_iter}-1$}
    \STATE $w = A Q[:, k]$
    \FOR{$j = 0, \dots, k$}
        \STATE $H[j, k] = Q[:, j]^\top w$
        \STATE $w = w - H[j, k] Q[:, j]$
    \ENDFOR
    \STATE $H[k+1, k] = \|w\|$
    \IF{$H[k+1, k] < \text{tol}$}
        \STATE \textbf{break}
    \ENDIF
    \STATE $Q[:, k+1] = w / H[k+1, k]$
    \STATE Solve the least-squares problem $H[:k+2, :k+1] y = \beta e_1$
    \STATE Compute residual $\text{residual} = H[k+1, k] |y_{-1}|$
    \IF{$\text{residual} < \text{tol}$}
        \STATE \textbf{break}
    \ENDIF
\ENDFOR
\STATE $x = x_0 + Q[:, :k+1] y$
\RETURN $x$
\end{algorithmic}
\end{algorithm}
\section{Tests and Comparison}

For numerical tests, we set $b=\begin{pmatrix} 1 \\ 1 \\ \vdots \\ 1 \end{pmatrix}$, tolerance $tol=10^{-6}$.
\subsection*{Steepest Gradient Descent}
\begin{lstlisting}[caption={Steepest Gradient Descent (SDG)}, label={lst:sdg}]
def SDG(A, b, iter_max, tol, x=0):
    r = b.copy()
    iter = 0
    err = np.linalg.norm(r)
    L = [np.linalg.norm(r)]
    while iter < iter_max and err > tol:
        iter += 1
        Ar = A @ r  # Ar_k
        err2 = np.dot(r, r)
        alpha = err2 / np.dot(Ar, r)
        x += alpha * r
        r -= alpha * Ar
        err = np.sqrt(err2)
        L.append(err)
    return L, iter
\end{lstlisting}

\subsection*{Conjugate Gradient}
\begin{lstlisting}[caption={Conjugate Gradient (CG)}, label={lst:cg}]
def CG(A, b, iter_max, tol, x=0):
    r = b.copy()
    p = b.copy()
    iter = 0
    err = np.linalg.norm(r)
    L = [err]
    while iter < iter_max and err > tol:
        iter += 1
        rk2 = np.dot(r, r)
        alpha = rk2 / np.dot(A @ p, p)
        x += alpha * p
        r -= alpha * A @ p
        beta = np.dot(r, r) / rk2
        p = r + beta * p
        err = np.linalg.norm(r)
        L.append(err)
    return L, iter, x
\end{lstlisting}

\subsection*{Arnoldi (FOM)}
\begin{lstlisting}[caption={Arnoldi (FOM)}, label={lst:fom}]
def FOM(A, b, m, tol, x=0):
    residuals = []
    r = b.copy()
    e1 = np.zeros(m)
    e1[0] = np.linalg.norm(r)
    V = np.zeros((len(b), m + 1))
    V[:, 0] = r / e1[0]
    H = np.zeros((m + 1, m))
    for j in range(0, m):
        w = A @ V[:, j]
        for i in range(0, j + 1):
            H[i, j] = np.dot(V[:, i], w)
            w -= H[i, j] * V[:, i]
        H[j + 1, j] = np.linalg.norm(w)
        if H[j + 1, j] == 0:
            break
        V[:, j + 1] = w / H[j + 1, j]
        x_m = V[:, :j + 1] @ np.linalg.solve(H[:j + 1, :j + 1], e1[:j + 1])
        residual = np.linalg.norm(b - A @ x_m)
        if residual < tol:
            return residuals, x_m
        residuals.append(residual)
    return residuals, x_m
\end{lstlisting}

%\subsection*{Arnoldi Recursive (FOM Recursive)}
%\begin{lstlisting}[caption={Arnoldi Recursive (FOM Recursive)}, label={lst:fom_recursive}]
%def FOM_recursive(A, b, m, tol, x=0):
%    residuals = []
%    r = b.copy()
%    e1 = np.zeros(m)
%    e1[0] = np.linalg.norm(r)
%    V = np.zeros((len(b), m + 1))
%    V[:, 0] = r / e1[0]
%    H = np.zeros((m + 1, m))
%    for j in range(0, m):
%        w = A @ V[:, j]
%        for i in range(0, j + 1):
%            H[i, j] = np.dot(V[:, i], w)
%            w -= H[i, j] * V[:, i]
%        H[j + 1, j] = np.linalg.norm(w)
%        if H[j + 1, j] == 0:
%            break
%        V[:, j + 1] = w / H[j + 1, j]
%        Y = FOM_recursive(H[:j + 1, :j + 1], e1[:j + 1], j + 1, tol, x=0)
%        residual = np.linalg.norm(b - A @ V[:, :j + 1] @ Y)
%        if residual < tol:
%            return residuals
%        residuals.append(residual)
%    return residuals
%\end{lstlisting}

\subsection*{Generalized Minimum Residual (GMRES)}
\begin{lstlisting}[caption={Generalized Minimum Residual (GMRES)}, label={lst:gmres}]
def gmres(A, b, m, tol, x=0):
    residuals = []
    r = b.copy()
    e1 = np.zeros(m + 1)
    e1[0] = np.linalg.norm(r)
    V = np.zeros((len(b), m + 1))
    V[:, 0] = r / e1[0]
    H = np.zeros((m + 1, m))
    for j in range(0, m):
        w = A @ V[:, j]
        for i in range(0, j + 1):
            H[i, j] = np.dot(V[:, i], w)
            w -= H[i, j] * V[:, i]
        H[j + 1, j] = np.linalg.norm(w)
        if H[j + 1, j] == 0:
            break
        V[:, j + 1] = w / H[j + 1, j]
        x_m = V[:, :j + 1] @ np.linalg.lstsq(H[:j + 2, :j + 1], e1[:j + 2], rcond=None)[0]
        residual = np.linalg.norm(b - A @ x_m)
        if residual < tol:
            return residuals, x_m
        residuals.append(residual)
    return residuals, x_m
\end{lstlisting}

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{DG.png}
        \caption{Steepest Descent}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{cg.png}
        \caption{Conjugate Gradient}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{fom.png}
        \caption{FOM}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{gmres.png}
        \caption{GMRES}
    \end{subfigure}
    \caption{Residual Errors as a Function of Iterations for Different Algorithms}
    \label{fig:convergence_graphs}
\end{figure}


The following table summarizes the performance of the algorithms, considering execution time, iteration counts, and convergence criteria.

\begin{table}[h!]
    \centering
    \begin{tabular}{@{} l c c c @{}}
        \toprule
        \textbf{Algorithm} & \textbf{Time Execution (s)} & \textbf{Iterations Max} & \textbf{Convergence Achieved} \\ 
        \midrule
        Steepest Descent  & 184.61 & 4001  & No \\ 
        Conjugate Gradient        & 38.41 &  90&  Yes \\ 
        FOM              & 8.56  & 82 &  Yes\\ 
        GMRes                   &  8.06 & 82 &  Yes \\
        \bottomrule
    \end{tabular}
    \caption{Comparison of algorithms based on execution time, iterations, and performance.}
    \label{tab:algo_comparison}
\end{table}
\begin{remark}
    Those results depends also on the vector $b$, we found different convervenge rate by considering a random generated vector $b$
\end{remark}
\begin{remark}
    We could also try implementing a recursive function and call the FOM or GMRES method within  FOM or GMRES itself respectively, instead of solving the system by inverting the matrix \( H \) using \inlinecode{\texttt{np.linalg.solve}} or solving the least squares problem with \inlinecode{\texttt{np.linalg.lstsq}}.
\end{remark}
\section{Conclusion and Perspectives}
In this report, we compared several iterative methods for solving linear systems. The main findings are:
\begin{itemize}
    \item Steepest Descent is simple but exhibits slow convergence, especially for ill-conditioned systems.
    \item Conjugate Gradient converges efficiently for symmetric positive-definite matrices.
    \item Krylov-based methods (FOM and GMRES) perform well for non-symmetric or general matrices, with GMRES being more robust for systems with larger condition numbers.
\end{itemize}

Future work could include:
\begin{itemize}
    \item Analyzing preconditioning techniques to accelerate convergence.
    \item Extending comparisons to parallel and distributed implementations.
    \item Applying these methods to real-world problems in engineering and data science.
\end{itemize}

\end{document}
